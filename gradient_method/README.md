#Grandient method

##1.Summary

    It is the simplest and the most commonly optimization method. It is easy to achieve. Only when the objective 
    
    function is a convex function, the solution of the gradient descent method is a global solution. Usually, the 

    the solution of the gradient descent method isn't a global solution.
    
##2.Process
    
    BGD(Batch gradient descent method):
    
    
    SGD(Stochastic gradient descent method):
    
##3.Experiment

    The number of sample is 100, The dimension is 3.
<table>
<tr>
<td> Method name </td> <td> Iteration number</td> <td> Calculation time </td>
</tr>
<tr>
<td> BGD </td> <td> 500 </td> <td> 0.021650905029 s </td>
<tr>
<td> SGD </td> <td> 150 </td> <td> 0.176577097458 s </td>

</tr>
</table>
    
    
